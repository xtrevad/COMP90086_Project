{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set saved to: split_train.csv\n",
      "Validation set saved to: split_val.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def calculate_stats(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=100, num_workers=0, shuffle=False)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Create a tqdm progress bar\n",
    "    pbar = tqdm(total=total_samples, desc=\"Calculating Stats\", unit=\"sample\")\n",
    "    \n",
    "    for batch in loader:\n",
    "        images = batch[0]  # Assuming images are always the first element\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(batch_samples)\n",
    "   \n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    \n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def save_stats(mean, std, filename):\n",
    "    data = {\n",
    "        'train_mean': mean.tolist(),\n",
    "        'train_std': std.tolist()\n",
    "    }\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def split_csv(file_path, train_ratio=0.9, random_state=42):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Split the data into train and validation sets\n",
    "    train_data, val_data = train_test_split(df, test_size=(1-train_ratio), random_state=random_state)\n",
    "    \n",
    "    # Generate new file names\n",
    "    base_name = file_path.rsplit('.', 1)[0]\n",
    "    train_file = f\"split_train.csv\"\n",
    "    val_file = f\"split_val.csv\"\n",
    "    \n",
    "    # Save the split datasets\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    val_data.to_csv(val_file, index=False)\n",
    "    \n",
    "    print(f\"Training set saved to: {train_file}\")\n",
    "    print(f\"Validation set saved to: {val_file}\")\n",
    "    \n",
    "    return train_file, val_file\n",
    "\n",
    "# Usage example\n",
    "file_path = \"./COMP90086_2024_Project_train/train.csv\"\n",
    "train_file, val_file = split_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/7680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bensa\\AppData\\Local\\Temp\\ipykernel_22924\\354567196.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_name = str(row[0])\n",
      "Processing images: 100%|██████████| 7680/7680 [01:45<00:00, 72.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image statistics saved to dataset_stats/full_quantized.json\n",
      "Mean: [118.6946178  111.99958266 103.1688242 ]\n",
      "Std: [68.48892217 57.7008792  47.95330727]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/6912 [00:00<?, ?it/s]C:\\Users\\bensa\\AppData\\Local\\Temp\\ipykernel_22924\\354567196.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_name = str(row[0])\n",
      "Processing images: 100%|██████████| 6912/6912 [01:35<00:00, 72.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image statistics saved to dataset_stats/split_quantized.json\n",
      "Mean: [118.68824203 111.95225786 103.06642011]\n",
      "Std: [68.46407555 57.64863105 47.88215884]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/7680 [00:00<?, ?it/s]C:\\Users\\bensa\\AppData\\Local\\Temp\\ipykernel_22924\\354567196.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_name = str(row[0])\n",
      "Processing images: 100%|██████████| 7680/7680 [01:41<00:00, 75.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image statistics saved to dataset_stats/full.json\n",
      "Mean: [119.21526968 112.51073973 103.68285506]\n",
      "Std: [69.10808986 58.16075209 48.56616013]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/6912 [00:00<?, ?it/s]C:\\Users\\bensa\\AppData\\Local\\Temp\\ipykernel_22924\\354567196.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_name = str(row[0])\n",
      "Processing images: 100%|██████████| 6912/6912 [01:35<00:00, 72.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image statistics saved to dataset_stats/split.json\n",
      "Mean: [119.20886585 112.46371131 103.58079217]\n",
      "Std: [69.08458836 58.10946274 48.49390902]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def calculate_image_stats(csv_file, csv_name, img_dir, use_quantized):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Initialize variables for mean and std calculation\n",
    "    sum_means = np.zeros(3)\n",
    "    sum_stds = np.zeros(3)\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through all images\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
    "        img_name = str(row[0])\n",
    "        if use_quantized:\n",
    "            img_path = os.path.join(img_dir, f\"quantized/{img_name}_quantized.jpg\")\n",
    "        else:\n",
    "            img_path = os.path.join(img_dir, f\"{img_name}_original.jpg\")\n",
    "        \n",
    "        # Skip augmented images\n",
    "        if \"_flipped\" in img_path or \"_zoomed\" in img_path:\n",
    "            continue\n",
    "\n",
    "        # Read and process the image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not read image {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Calculate mean and std for each channel\n",
    "        means = np.mean(image, axis=(0, 1))\n",
    "        stds = np.std(image, axis=(0, 1))\n",
    "        \n",
    "        sum_means += means\n",
    "        sum_stds += stds\n",
    "        count += 1\n",
    "\n",
    "    # Calculate final mean and std\n",
    "    final_mean = sum_means / count\n",
    "    final_std = sum_stds / count\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": final_mean.tolist(),\n",
    "        \"std\": final_std.tolist()\n",
    "    }\n",
    "\n",
    "    quant = \"_quantized\" if use_quantized else \"\"\n",
    "    output_file = f\"dataset_stats/{csv_name}{quant}.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(stats, f, indent=4)\n",
    "\n",
    "    print(f\"Image statistics saved to {output_file}\")\n",
    "    print(f\"Mean: {final_mean}\")\n",
    "    print(f\"Std: {final_std}\")\n",
    "\n",
    "img_dir = './preprocessed_images/train'\n",
    "main_train = './COMP90086_2024_Project_train/train.csv'\n",
    "split_train = 'split_train.csv'\n",
    "\n",
    "for quant in [True, False]:\n",
    "    calculate_image_stats(main_train, \"full\", img_dir, quant)\n",
    "    calculate_image_stats(split_train, \"split\", img_dir, quant)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
