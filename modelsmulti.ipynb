{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import EfficientNet_V2_S_Weights, convnext_base\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import platform\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "class StabilityDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, augment=False, use_quantized=False, additional_columns=None, target_column=None, balance_dataset=False, reference_csv=None):\n",
    "        self.stability_data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.use_quantized = use_quantized\n",
    "        self.additional_columns = additional_columns or []\n",
    "        self.target_column = target_column\n",
    "        self.image_files = self._get_image_files()\n",
    "        self.feature_categories = self._get_feature_categories()\n",
    "\n",
    "        # Use reference_csv for label mapping if provided (for test set)\n",
    "        if reference_csv and target_column:\n",
    "            reference_data = pd.read_csv(reference_csv)\n",
    "            self.unique_labels = sorted(reference_data[target_column].unique())\n",
    "        elif target_column and target_column in self.stability_data.columns:\n",
    "            self.unique_labels = sorted(self.stability_data[target_column].unique())\n",
    "        else:\n",
    "            self.unique_labels = None\n",
    "\n",
    "        if self.unique_labels:\n",
    "            self.label_to_index = {label: index for index, label in enumerate(self.unique_labels)}\n",
    "            self.index_to_label = {index: label for label, index in self.label_to_index.items()}\n",
    "        else:\n",
    "            self.label_to_index = None\n",
    "            self.index_to_label = None\n",
    "\n",
    "        if balance_dataset and self.target_column is not None:\n",
    "            self._balance_dataset()\n",
    "        \n",
    "        if balance_dataset and self.target_column is not None:\n",
    "            self._balance_dataset()\n",
    "\n",
    "    def _balance_dataset(self):\n",
    "        if self.target_column is None:\n",
    "            return\n",
    "\n",
    "        # Count occurrences of each class\n",
    "        class_counts = self.stability_data[self.target_column].value_counts()\n",
    "        min_class_count = class_counts.min()\n",
    "\n",
    "        # Undersample each class\n",
    "        balanced_data = []\n",
    "        for class_label in class_counts.index:\n",
    "            class_data = self.stability_data[self.stability_data[self.target_column] == class_label]\n",
    "            balanced_data.append(class_data.sample(min_class_count, replace=False))\n",
    "\n",
    "        # Combine the balanced classes\n",
    "        self.stability_data = pd.concat(balanced_data).reset_index(drop=True)\n",
    "\n",
    "        # Update image files based on the balanced dataset\n",
    "        self.image_files = self._get_image_files()\n",
    "\n",
    "\n",
    "    def _get_image_files(self):\n",
    "        image_files = []\n",
    "        for idx, row in self.stability_data.iterrows():\n",
    "            img_name = str(row.iloc[0])\n",
    "            if self.use_quantized:\n",
    "                image_files.append(f\"quantized/{img_name}_quantized.jpg\")\n",
    "                if self.augment:\n",
    "                    image_files.extend([\n",
    "                        f\"quantized/{img_name}_flipped_quantized.jpg\",\n",
    "                        f\"quantized/{img_name}_zoomed_quantized.jpg\",\n",
    "                        f\"quantized/{img_name}_zoomed_flipped_quantized.jpg\"\n",
    "                    ])\n",
    "            else:\n",
    "                image_files.append(f\"{img_name}_original.jpg\")\n",
    "                if self.augment:\n",
    "                    image_files.extend([\n",
    "                        f\"{img_name}_flipped.jpg\",\n",
    "                        f\"{img_name}_zoomed.jpg\",\n",
    "                        f\"{img_name}_zoomed_flipped.jpg\"\n",
    "                    ])\n",
    "        return image_files\n",
    "\n",
    "    def _get_feature_categories(self):\n",
    "        feature_categories = {}\n",
    "        for col in self.additional_columns:\n",
    "            if col in self.stability_data.columns:\n",
    "                unique_values = self.stability_data[col].unique()\n",
    "                feature_categories[col] = {\n",
    "                    'num_categories': len(unique_values),\n",
    "                    'value_to_index': {val: idx for idx, val in enumerate(unique_values)}\n",
    "                }\n",
    "        if self.target_column and self.target_column in self.stability_data.columns:\n",
    "            unique_values = self.stability_data[self.target_column].unique()\n",
    "            feature_categories[self.target_column] = {\n",
    "                'num_categories': len(unique_values),\n",
    "                'value_to_index': {val: idx for idx, val in enumerate(unique_values)}\n",
    "            }\n",
    "        return feature_categories\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_idx = idx // 4 if self.augment else idx\n",
    "        image_id = self.stability_data.iloc[original_idx, 0]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose((2, 0, 1))).float() / 255.0\n",
    "\n",
    "        additional_data = []\n",
    "        for col in self.additional_columns:\n",
    "            if col in self.stability_data.columns:\n",
    "                value = self.stability_data.iloc[original_idx][col]\n",
    "                index = self.feature_categories[col]['value_to_index'][value]\n",
    "                additional_data.append(torch.tensor(index, dtype=torch.long))\n",
    "\n",
    "        if self.target_column and self.target_column in self.stability_data.columns:\n",
    "            target_value = self.stability_data.iloc[original_idx][self.target_column]\n",
    "            target_index = self.label_to_index[target_value]\n",
    "            return (image, image_id, torch.tensor(target_index, dtype=torch.long), *additional_data)\n",
    "        else:\n",
    "            return (image, image_id, *additional_data)\n",
    "\n",
    "\n",
    "    def get_feature_dimensions(self):\n",
    "        return {col: info['num_categories'] for col, info in self.feature_categories.items() if col != self.target_column}\n",
    "\n",
    "    def get_target_dimension(self):\n",
    "        if self.target_column and self.target_column in self.stability_data.columns:\n",
    "            return len(self.unique_labels)\n",
    "        return None\n",
    "\n",
    "    def get_original_label(self, index):\n",
    "        if self.index_to_label is None:\n",
    "            raise ValueError(\"Label mapping is not available\")\n",
    "        return self.index_to_label[index]\n",
    "\n",
    "class StabilityPredictor(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3, additional_features=None):\n",
    "        super(StabilityPredictor, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16  # You can adjust this value\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine image features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one (Dropout + Linear layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Process the image through EfficientNet\n",
    "        x = self.efficientnet.features(x)\n",
    "        x = self.efficientnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate image features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientAttentionNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3, additional_features=None):\n",
    "        super(EfficientAttentionNet, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights for EfficientNet V2 Small\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "        # Spatial attention module\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=7)\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine image features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one (Dropout + Linear layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Pass through the feature extractor (EfficientNet backbone) until the last feature map\n",
    "        features = self.efficientnet.features(x)  # Extract convolutional features\n",
    "        \n",
    "        # Apply spatial attention module to the feature maps\n",
    "        features = self.spatial_attention(features)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.efficientnet.avgpool(features)\n",
    "        \n",
    "        # Flatten the pooled features\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate image features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel-wise max and average pooling (along spatial dimensions)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)\n",
    "        attention_map = self.sigmoid(self.conv(concat))\n",
    "        return x * attention_map\n",
    "\n",
    "class EfficientChannelAttentionNet(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.0, additional_features=None):\n",
    "        super(EfficientChannelAttentionNet, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights for EfficientNet V2 Small\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "        # Add channel attention modules after specific layers in the EfficientNet backbone\n",
    "        self.channel_attention1 = ChannelAttentionModule(in_planes=24)  # After first block (features[1])\n",
    "        self.channel_attention2 = ChannelAttentionModule(in_planes=48)  # After second block (features[2])\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine image features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one (Dropout + Linear layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Pass input through the first few layers of EfficientNet\n",
    "        x = self.efficientnet.features[0](x)  # Initial convolution and stem\n",
    "        x = self.efficientnet.features[1](x)  # First block (channels: 24)\n",
    "        x = self.channel_attention1(x)  # Apply channel attention after the first block\n",
    "        \n",
    "        x = self.efficientnet.features[2](x)  # Second block (channels: 48)\n",
    "        x = self.channel_attention2(x)  # Apply channel attention after the second block\n",
    "        \n",
    "        # Continue with the rest of the EfficientNet layers\n",
    "        for i in range(3, len(self.efficientnet.features)):\n",
    "            x = self.efficientnet.features[i](x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.efficientnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate image features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        attention = self.sigmoid(avg_out + max_out)\n",
    "        return x * attention\n",
    "    \n",
    "class ConvnextPredictor(nn.Module):\n",
    "    def __init__(self, num_classes=6, freeze_layers=True, additional_features=None):\n",
    "        super(ConvnextPredictor, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights\n",
    "        weights = models.convnext.ConvNeXt_Base_Weights.DEFAULT\n",
    "        self.convnextnet = convnext_base(weights=weights)\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.convnextnet.classifier[2].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16  # You can adjust this value\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine ConvNeXt features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(num_ftrs),  # ConvNeXt uses LayerNorm instead of BatchNorm\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "        if freeze_layers:\n",
    "            print('Layers frozen!')\n",
    "            # Freeze ConvNeXt backbone layers for quicker fine-tuning training\n",
    "            for param in self.convnextnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Only unfreeze the classifier layers and the combined layer\n",
    "            for param in self.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.combined_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "            for embedding_layer in self.embedding_layers.values():\n",
    "                for param in embedding_layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Pass through ConvNeXt backbone\n",
    "        x = self.convnextnet.features(x)\n",
    "        x = self.convnextnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate ConvNeXt features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def colour_quantisation(image, k=20):\n",
    "    # Convert the image to 2D pixel array\n",
    "    pixels = np.float32(image.reshape(-1, 3))\n",
    "\n",
    "    # Define criteria for K-Means (stop after 10 iter or if accuracy reaches 1.0)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    _, labels, palette = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    # Convert back to 8-bit values\n",
    "    quantised = np.uint8(palette)[labels.flatten()]\n",
    "\n",
    "    # Reshape the image to original dimensions\n",
    "    quantised = quantised.reshape(image.shape)\n",
    "    \n",
    "    return quantised\n",
    "\n",
    "def train_model_full_dataset(model, train_loader, criterion, optimizer, scheduler, num_epochs, device, lr_schedule):\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the learning rate for each epoch\n",
    "    def get_lr(epoch):\n",
    "        return lr_schedule.get(epoch, lr_schedule[max(k for k in lr_schedule.keys() if k <= epoch)])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Set the learning rate for this epoch\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = get_lr(epoch)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device, is_training=True)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print('-' * 60)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    lr_schedule = {0: optimizer.param_groups[0]['lr']}  # Initial learning rate\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device, is_training=True)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer, device, is_training=False)\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # If learning rate changed, add to schedule\n",
    "        if new_lr != old_lr:\n",
    "            lr_schedule[epoch + 1] = new_lr\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(f'Learning Rate: {new_lr:.6f}')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "\n",
    "    return model, best_epoch, best_val_loss, lr_schedule\n",
    "\n",
    "def run_epoch(model, data_loader, criterion, optimizer, device, is_training=True):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\" if is_training else \"Validating\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        additional_inputs = [feature.to(device) for feature in batch[3:]]  # Change this from batch[2:] to batch[3:]\n",
    "        \n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs, *additional_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100. * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_acc = 100. * correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            ids = batch[1]\n",
    "            additional_inputs = [feature.to(device) for feature in batch[2:]]\n",
    "            outputs = model(inputs, *additional_inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            image_ids.extend(ids.numpy())\n",
    "    \n",
    "    return predictions, image_ids\n",
    "\n",
    "def calculate_stats(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=100, num_workers=get_optimal_num_workers(), shuffle=False)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Create a tqdm progress bar\n",
    "    pbar = tqdm(total=total_samples, desc=\"Calculating Stats\", unit=\"sample\")\n",
    "    \n",
    "    for batch in loader:\n",
    "        images = batch[0]  # Assuming images are always the first element\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(batch_samples)\n",
    "   \n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    \n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def check_label_consistency(train_csv, predictions_csv, target_column):\n",
    "    train_data = pd.read_csv(train_csv)\n",
    "    pred_data = pd.read_csv(predictions_csv)\n",
    "    \n",
    "    train_labels = set(train_data[target_column].unique())\n",
    "    pred_labels = set(pred_data[target_column].unique())\n",
    "    \n",
    "    if train_labels == pred_labels:\n",
    "        print(\"Labels are consistent between training set and predictions.\")\n",
    "    else:\n",
    "        print(\"Warning: Label mismatch detected!\")\n",
    "        print(\"Training labels:\", train_labels)\n",
    "        print(\"Prediction labels:\", pred_labels)\n",
    "        print(\"Difference:\", train_labels.symmetric_difference(pred_labels))\n",
    "\n",
    "\n",
    "def save_split_and_stats(train_indices, val_indices, train_mean, train_std, filename):\n",
    "    data = {\n",
    "        'train_indices': train_indices,\n",
    "        'val_indices': val_indices,\n",
    "        'train_mean': train_mean.tolist(),\n",
    "        'train_std': train_std.tolist()\n",
    "    }\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_split_and_stats(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return (\n",
    "        data['train_indices'],\n",
    "        data['val_indices'],\n",
    "        torch.tensor(data['train_mean']),\n",
    "        torch.tensor(data['train_std'])\n",
    "    )\n",
    "\n",
    "# Windows can't do multicore processing\n",
    "def get_optimal_num_workers():\n",
    "    if platform.system() == 'Windows':\n",
    "        return 0\n",
    "    else:\n",
    "        return multiprocessing.cpu_count()\n",
    "\n",
    "def train_and_save(config, use_full_dataset=False, run_predictions=False):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Check if we should load existing split and stats\n",
    "    full_data_suffix = \"_full\" if use_full_dataset else \"\"\n",
    "    split_stats_file = f'split_and_stats{full_data_suffix}.json'\n",
    "    if config['use_existing_split'] and os.path.exists(split_stats_file):\n",
    "        print(f\"Loading existing split and stats from {split_stats_file}\")\n",
    "        train_indices, val_indices, train_mean, train_std = load_split_and_stats(split_stats_file)\n",
    "    else:\n",
    "        # Create a base dataset without augmentation for splitting and stats calculation\n",
    "        base_dataset = StabilityDataset(csv_file=config['train_csv'], \n",
    "                                        img_dir=config['train_img_dir'], \n",
    "                                        transform=transforms.ToTensor(),\n",
    "                                        augment=False,\n",
    "                                        use_quantized=config['use_quantized'],\n",
    "                                        target_column=config['target_column'],\n",
    "                                        additional_columns=config['additional_columns'],\n",
    "                                        balance_dataset=config['balance_dataset'])\n",
    "\n",
    "        # Calculate statistics for the entire dataset\n",
    "        print(\"Calculating dataset statistics...\")\n",
    "        train_mean, train_std = calculate_stats(base_dataset)\n",
    "        print(f\"Dataset mean: {train_mean}\")\n",
    "        print(f\"Dataset std: {train_std}\")\n",
    "\n",
    "        if use_full_dataset:\n",
    "            train_indices = list(range(len(base_dataset)))\n",
    "            val_indices = []\n",
    "        else:\n",
    "            # Split dataset into train and validation\n",
    "            dataset_size = len(base_dataset)\n",
    "            indices = list(range(dataset_size))\n",
    "            np.random.shuffle(indices)\n",
    "            split = int(np.floor(config['val_ratio'] * dataset_size))\n",
    "            train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Save split and stats\n",
    "        save_split_and_stats(train_indices, val_indices, train_mean, train_std, split_stats_file)\n",
    "        print(f\"Split and stats saved to {split_stats_file}\")\n",
    "\n",
    "    # Create transforms\n",
    "    normalize_transform = transforms.Normalize(mean=train_mean, std=train_std)\n",
    "    \n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform,\n",
    "    ])\n",
    "\n",
    "    # Create datasets with appropriate transforms\n",
    "    full_dataset = StabilityDataset(csv_file=config['train_csv'], \n",
    "                                    img_dir=config['train_img_dir'], \n",
    "                                    transform=base_transform,\n",
    "                                    augment=config['use_augmentation'],\n",
    "                                    use_quantized=config['use_quantized'],\n",
    "                                    additional_columns=config['additional_columns'],\n",
    "                                    target_column=config['target_column'],\n",
    "                                    balance_dataset=config['balance_dataset'])\n",
    "\n",
    "    # Get the number of categories for each additional feature\n",
    "    additional_features = full_dataset.get_feature_dimensions()\n",
    "    num_classes = full_dataset.get_target_dimension()\n",
    "\n",
    "    # Apply the split, ensuring augmented images stay with their original counterparts\n",
    "    if config['use_augmentation']:\n",
    "        train_indices = [i for idx in train_indices for i in range(idx * 4, (idx + 1) * 4)]\n",
    "        val_indices = [i for idx in val_indices for i in range(idx * 4, (idx + 1) * 4)]\n",
    "    \n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    \n",
    "    if not use_full_dataset:\n",
    "        val_dataset = Subset(full_dataset, val_indices[:len(val_indices)//4])  # Only use original images for validation\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=get_optimal_num_workers())\n",
    "\n",
    "    # Create data loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=get_optimal_num_workers())\n",
    "\n",
    "    # Initialize model, criterion, optimizer, and scheduler\n",
    "    if config['model'] == 'StabilityPredictor':\n",
    "        model = StabilityPredictor(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "    elif config['model'] == 'EfficientAttentionNet':\n",
    "        model = EfficientAttentionNet(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "    elif config['model'] == 'EfficientChannelAttentionNet':\n",
    "        model = EfficientChannelAttentionNet(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "    elif config['model'] == 'ConvnextPredictor':\n",
    "        model = ConvnextPredictor(num_classes=num_classes, freeze_layers=config['freeze_layers'], additional_features=additional_features)\n",
    "    else:\n",
    "        print('Unrecognised model in config. Defaulting to StabilityPredictor (EfficientNet)')\n",
    "        model = StabilityPredictor(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "\n",
    "    print('Model: ' + config['model'])\n",
    "    print('Using quantized images: ' + str(config['use_quantized']))\n",
    "    print('Additional features:', ', '.join(f\"{k}: {v} categories\" for k, v in additional_features.items()))\n",
    "    print(f'Target feature: {config[\"target_column\"]} ({num_classes} categories)')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if use_full_dataset:\n",
    "        # Load training parameters from JSON file\n",
    "        training_params_file = config.get('training_params_file', f\"{config['model']}_training_params.json\")\n",
    "        training_params = load_training_params(training_params_file)\n",
    "        num_epochs = training_params['epochs']\n",
    "        lr_schedule = training_params['lr_schedule']\n",
    "        print(f\"Loaded training parameters from {training_params_file}\")\n",
    "        print(f\"Epochs: {num_epochs}\")\n",
    "        print(f\"Learning rate schedule: {lr_schedule}\")\n",
    "\n",
    "        # Use the loaded learning rate schedule\n",
    "        initial_lr = list(lr_schedule.values())[0]\n",
    "        optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "        # We don't need a scheduler anymore, as we're manually setting the learning rate in train_model_full_dataset\n",
    "        scheduler = None\n",
    "        \n",
    "        # Train model without validation\n",
    "        print('Training on full dataset...')\n",
    "        model = train_model_full_dataset(model, train_loader, criterion, optimizer, scheduler, num_epochs, device, lr_schedule)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=config['lr_factor'], patience=config['lr_patience'], verbose=True)\n",
    "        \n",
    "        # Train model with validation\n",
    "        print('Training with validation...')\n",
    "        model, best_epoch, best_val_loss, lr_schedule = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                            num_epochs=config['num_epochs'], patience=config['early_stopping_patience'], device=device)\n",
    "\n",
    "        # Save training parameters\n",
    "        training_params = {\n",
    "            \"epochs\": best_epoch + 1,  # Add 1 because epochs are 0-indexed\n",
    "            \"initial_lr\": config['learning_rate'],\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"lr_schedule\": {str(k): v for k, v in lr_schedule.items()},  # Convert keys to strings for JSON\n",
    "            \"lr_factor\": config['lr_factor'],\n",
    "            \"lr_patience\": config['lr_patience'],\n",
    "            \"early_stopping_patience\": config['early_stopping_patience']\n",
    "        }\n",
    "        training_params_file = config['training_params_file']\n",
    "        save_training_params(training_params_file, training_params)\n",
    "\n",
    "    # Prediction on test set (if run_predictions is True)\n",
    "    if run_predictions:\n",
    "        test_dataset = StabilityDataset(csv_file=config['test_csv'],\n",
    "                                        img_dir=config['test_img_dir'],\n",
    "                                        transform=base_transform,\n",
    "                                        augment=False,\n",
    "                                        use_quantized=config['use_quantized'],\n",
    "                                        additional_columns=config['additional_columns'],\n",
    "                                        target_column=config['target_column'],\n",
    "                                        reference_csv=config['train_csv'])  # Use train_csv as reference for label mapping\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=get_optimal_num_workers())\n",
    "\n",
    "        predictions, image_ids = predict(model, test_loader, device)\n",
    "\n",
    "        # Save predictions to CSV\n",
    "        model_type = \"full\" if use_full_dataset else \"val\"\n",
    "        predictions_save_path = f\"{model_type}_predictions.csv\"\n",
    "        with open(predictions_save_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['id', config['target_column']])\n",
    "            for img_id, pred in zip(image_ids, predictions):\n",
    "                original_label = test_dataset.get_original_label(pred)\n",
    "                writer.writerow([int(img_id), int(original_label)])\n",
    "        print(f\"Predictions saved to {predictions_save_path}\")\n",
    "\n",
    "        # Check label consistency\n",
    "        check_label_consistency(config['train_csv'], predictions_save_path, config['target_column'])\n",
    "\n",
    "\n",
    "def save_training_params(file_path, params):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "    print(f\"Training parameters saved to {file_path}\")\n",
    "\n",
    "def load_training_params(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    # Convert epoch numbers to integers in lr_schedule\n",
    "    if 'lr_schedule' in params:\n",
    "        params['lr_schedule'] = {int(k): v for k, v in params['lr_schedule'].items()}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Set config values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Files\n",
    "    'train_csv': './COMP90086_2024_Project_train/train.csv',\n",
    "    'train_img_dir': './preprocessed_images/train',\n",
    "    'test_csv': './COMP90086_2024_Project_test/test.csv',\n",
    "    'test_img_dir': './preprocessed_images/test',\n",
    "    'training_params_file': 'training_params.json',\n",
    "\n",
    "    # Training parameters\n",
    "    'model': 'EfficientAttentionNet',\n",
    "    'target_column': 'stable_height',\n",
    "    'additional_columns': [],\n",
    "    'balance_dataset': False,\n",
    "    'use_augmentation': True,\n",
    "    'use_quantized': False,\n",
    "    'val_ratio': 0.05,\n",
    "    'batch_size': 48,\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'lr_factor': 0.5,\n",
    "    'lr_patience': 2,\n",
    "    'freeze_layers': False,\n",
    "    'num_epochs': 30,\n",
    "    'use_existing_split': True,\n",
    "    'early_stopping_patience': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train with validation set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing split and stats from split_and_stats.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientAttentionNet\n",
      "Using quantized images: False\n",
      "Additional features: \n",
      "Target feature: stable_height (6 categories)\n",
      "Training with validation...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdabe1cd16249729f445442bd902f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_save(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train on whole set and predict</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing split and stats from split_and_stats_full.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientAttentionNet\n",
      "Using quantized images: False\n",
      "Additional features: \n",
      "Target feature: stable_height (6 categories)\n",
      "Loaded training parameters from training_params.json\n",
      "Epochs: 7\n",
      "Learning rate schedule: {0: 0.0001, 8: 1e-05, 18: 1e-05}\n",
      "Training on full dataset...\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3320a0cb5ac44c1cbde971219be60911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5333, Train Acc: 31.78%\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 2/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd99f2e6e7ca4cff82aadb78278a391b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1433, Train Acc: 53.02%\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 3/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8c2a2b3d0c4f7b8e3a6ba09a3f3363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8663, Train Acc: 66.73%\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 4/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b813ca2d44cf41f4840436020c8feb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5880, Train Acc: 78.45%\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 5/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62724cf86692412db16f9fc384089bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3717, Train Acc: 86.76%\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 6/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3017be0a284b10b794bbd6e7e9ec99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2561, Train Acc: 91.34%\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 7/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb545e1e053433fba4b7b167772fad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1830, Train Acc: 93.84%\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Predictions saved to full_predictions.csv\n",
      "Labels are consistent between training set and predictions.\n"
     ]
    }
   ],
   "source": [
    "train_and_save(config, use_full_dataset=True, run_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./COMP90086_2024_Project_train/train.csv')\n",
    "pred = pd.read_csv('./full_predictions.csv')\n",
    "\n",
    "pred['true_label'] = train[config['target_column']]\n",
    "pred.to_csv('comparison.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
