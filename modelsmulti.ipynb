{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import EfficientNet_V2_S_Weights, convnext_base\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import platform\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "class StabilityDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, augment=False, use_quantized=False, additional_columns=None, target_column=None):\n",
    "        self.stability_data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.use_quantized = use_quantized\n",
    "        self.additional_columns = additional_columns or []\n",
    "        self.target_column = target_column\n",
    "        self.image_files = self._get_image_files()\n",
    "        self.feature_categories = self._get_feature_categories()\n",
    "\n",
    "    def _get_image_files(self):\n",
    "        image_files = []\n",
    "        for idx, row in self.stability_data.iterrows():\n",
    "            img_name = str(row[0])\n",
    "            if self.use_quantized:\n",
    "                image_files.append(f\"quantized/{img_name}_quantized.jpg\")\n",
    "                if self.augment:\n",
    "                    image_files.extend([\n",
    "                        f\"quantized/{img_name}_flipped_quantized.jpg\",\n",
    "                        f\"quantized/{img_name}_zoomed_quantized.jpg\",\n",
    "                        f\"quantized/{img_name}_zoomed_flipped_quantized.jpg\"\n",
    "                    ])\n",
    "            else:\n",
    "                image_files.append(f\"{img_name}_original.jpg\")\n",
    "                if self.augment:\n",
    "                    image_files.extend([\n",
    "                        f\"{img_name}_flipped.jpg\",\n",
    "                        f\"{img_name}_zoomed.jpg\",\n",
    "                        f\"{img_name}_zoomed_flipped.jpg\"\n",
    "                    ])\n",
    "        return image_files\n",
    "\n",
    "    def _get_feature_categories(self):\n",
    "        feature_categories = {}\n",
    "        for col in self.additional_columns + [self.target_column]:\n",
    "            unique_values = self.stability_data[col].unique()\n",
    "            feature_categories[col] = {\n",
    "                'num_categories': len(unique_values),\n",
    "                'value_to_index': {val: idx for idx, val in enumerate(unique_values)}\n",
    "            }\n",
    "        return feature_categories\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_idx = idx // 4 if self.augment else idx\n",
    "        target_value = self.stability_data.iloc[original_idx][self.target_column]\n",
    "        target_class = self.feature_categories[self.target_column]['value_to_index'][target_value]\n",
    "\n",
    "        # Get the image ID (assuming it's the first column in the CSV)\n",
    "        image_id = self.stability_data.iloc[original_idx, 0]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose((2, 0, 1))).float() / 255.0\n",
    "\n",
    "        # Get additional column values\n",
    "        additional_data = []\n",
    "        for col in self.additional_columns:\n",
    "            if col != self.target_column:\n",
    "                value = self.stability_data.iloc[original_idx][col]\n",
    "                index = self.feature_categories[col]['value_to_index'][value]\n",
    "                additional_data.append(torch.tensor(index, dtype=torch.long))\n",
    "\n",
    "        # Return image tensor first, followed by image_id, label, and additional data\n",
    "        return (image, image_id, torch.tensor(target_class, dtype=torch.long), *additional_data)\n",
    "\n",
    "\n",
    "    def get_feature_dimensions(self):\n",
    "        return {col: info['num_categories'] for col, info in self.feature_categories.items() if col != self.target_column}\n",
    "\n",
    "    def get_target_dimension(self):\n",
    "        return self.feature_categories[self.target_column]['num_categories']\n",
    "\n",
    "class StabilityPredictor(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3, additional_features=None):\n",
    "        super(StabilityPredictor, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16  # You can adjust this value\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine image features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one (Dropout + Linear layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Process the image through EfficientNet\n",
    "        x = self.efficientnet.features(x)\n",
    "        x = self.efficientnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate image features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientAttentionNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3, additional_features=None):\n",
    "        super(EfficientAttentionNet, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights for EfficientNet V2 Small\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "        # Spatial attention module\n",
    "        self.spatial_attention = SpatialAttentionModule(kernel_size=7)\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine image features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one (Dropout + Linear layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Pass through the feature extractor (EfficientNet backbone) until the last feature map\n",
    "        features = self.efficientnet.features(x)  # Extract convolutional features\n",
    "        \n",
    "        # Apply spatial attention module to the feature maps\n",
    "        features = self.spatial_attention(features)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.efficientnet.avgpool(features)\n",
    "        \n",
    "        # Flatten the pooled features\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate image features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel-wise max and average pooling (along spatial dimensions)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)\n",
    "        attention_map = self.sigmoid(self.conv(concat))\n",
    "        return x * attention_map\n",
    "\n",
    "class EfficientChannelAttentionNet(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.0, additional_features=None):\n",
    "        super(EfficientChannelAttentionNet, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights for EfficientNet V2 Small\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "        # Add channel attention modules after specific layers in the EfficientNet backbone\n",
    "        self.channel_attention1 = ChannelAttentionModule(in_planes=24)  # After first block (features[1])\n",
    "        self.channel_attention2 = ChannelAttentionModule(in_planes=48)  # After second block (features[2])\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine image features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one (Dropout + Linear layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Pass input through the first few layers of EfficientNet\n",
    "        x = self.efficientnet.features[0](x)  # Initial convolution and stem\n",
    "        x = self.efficientnet.features[1](x)  # First block (channels: 24)\n",
    "        x = self.channel_attention1(x)  # Apply channel attention after the first block\n",
    "        \n",
    "        x = self.efficientnet.features[2](x)  # Second block (channels: 48)\n",
    "        x = self.channel_attention2(x)  # Apply channel attention after the second block\n",
    "        \n",
    "        # Continue with the rest of the EfficientNet layers\n",
    "        for i in range(3, len(self.efficientnet.features)):\n",
    "            x = self.efficientnet.features[i](x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.efficientnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate image features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        attention = self.sigmoid(avg_out + max_out)\n",
    "        return x * attention\n",
    "    \n",
    "class ConvnextPredictor(nn.Module):\n",
    "    def __init__(self, num_classes=6, freeze_layers=True, additional_features=None):\n",
    "        super(ConvnextPredictor, self).__init__()\n",
    "\n",
    "        # Default pre-trained weights\n",
    "        weights = models.convnext.ConvNeXt_Base_Weights.DEFAULT\n",
    "        self.convnextnet = convnext_base(weights=weights)\n",
    "\n",
    "        # Get the number of input features to the final classifier layer\n",
    "        num_ftrs = self.convnextnet.classifier[2].in_features\n",
    "\n",
    "        # Embedding layers for additional features\n",
    "        self.additional_features = additional_features or {}\n",
    "        self.embedding_layers = nn.ModuleDict()\n",
    "        self.embedding_dim = 16  # You can adjust this value\n",
    "        total_embedding_dim = 0\n",
    "\n",
    "        for feature, num_categories in self.additional_features.items():\n",
    "            self.embedding_layers[feature] = nn.Embedding(num_categories, self.embedding_dim)\n",
    "            total_embedding_dim += self.embedding_dim\n",
    "\n",
    "        # Combine ConvNeXt features with embeddings\n",
    "        self.combined_layer = nn.Linear(num_ftrs + total_embedding_dim, num_ftrs)\n",
    "\n",
    "        # Replace the default classifier with a custom one\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(num_ftrs),  # ConvNeXt uses LayerNorm instead of BatchNorm\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "        if freeze_layers:\n",
    "            print('Layers frozen!')\n",
    "            # Freeze ConvNeXt backbone layers for quicker fine-tuning training\n",
    "            for param in self.convnextnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Only unfreeze the classifier layers and the combined layer\n",
    "            for param in self.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.combined_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "            for embedding_layer in self.embedding_layers.values():\n",
    "                for param in embedding_layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    def forward(self, x, *additional_inputs):\n",
    "        # Pass through ConvNeXt backbone\n",
    "        x = self.convnextnet.features(x)\n",
    "        x = self.convnextnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Process additional features through embedding layers\n",
    "        embeddings = []\n",
    "        for i, (feature, _) in enumerate(self.additional_features.items()):\n",
    "            embedding = self.embedding_layers[feature](additional_inputs[i])\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Concatenate ConvNeXt features with embeddings\n",
    "        if embeddings:\n",
    "            x = torch.cat([x] + embeddings, dim=1)\n",
    "            x = self.combined_layer(x)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def colour_quantisation(image, k=20):\n",
    "    # Convert the image to 2D pixel array\n",
    "    pixels = np.float32(image.reshape(-1, 3))\n",
    "\n",
    "    # Define criteria for K-Means (stop after 10 iter or if accuracy reaches 1.0)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    _, labels, palette = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    # Convert back to 8-bit values\n",
    "    quantised = np.uint8(palette)[labels.flatten()]\n",
    "\n",
    "    # Reshape the image to original dimensions\n",
    "    quantised = quantised.reshape(image.shape)\n",
    "    \n",
    "    return quantised\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device, is_training=True)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer, device, is_training=False)\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_epoch(model, data_loader, criterion, optimizer, device, is_training=True):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\" if is_training else \"Validating\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        additional_inputs = [feature.to(device) for feature in batch[3:]]  # Change this from batch[2:] to batch[3:]\n",
    "        \n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs, *additional_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100. * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_acc = 100. * correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            ids = batch[1]\n",
    "            additional_inputs = [feature.to(device) for feature in batch[3:]]\n",
    "            outputs = model(inputs, *additional_inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy() + 1)  # Add 1 to convert back to 1-6 range\n",
    "            image_ids.extend(ids.numpy())  # Convert tensor to numpy array\n",
    "    return predictions, image_ids\n",
    "\n",
    "def calculate_stats(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=100, num_workers=get_optimal_num_workers(), shuffle=False)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Create a tqdm progress bar\n",
    "    pbar = tqdm(total=total_samples, desc=\"Calculating Stats\", unit=\"sample\")\n",
    "    \n",
    "    for batch in loader:\n",
    "        images = batch[0]  # Assuming images are always the first element\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(batch_samples)\n",
    "   \n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    \n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def save_split_and_stats(train_indices, val_indices, train_mean, train_std, filename):\n",
    "    data = {\n",
    "        'train_indices': train_indices,\n",
    "        'val_indices': val_indices,\n",
    "        'train_mean': train_mean.tolist(),\n",
    "        'train_std': train_std.tolist()\n",
    "    }\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_split_and_stats(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return (\n",
    "        data['train_indices'],\n",
    "        data['val_indices'],\n",
    "        torch.tensor(data['train_mean']),\n",
    "        torch.tensor(data['train_std'])\n",
    "    )\n",
    "\n",
    "# Windows can't do multicore processing\n",
    "def get_optimal_num_workers():\n",
    "    if platform.system() == 'Windows':\n",
    "        return 0\n",
    "    else:\n",
    "        return multiprocessing.cpu_count()\n",
    "\n",
    "def train_and_save(config):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Check if we should load existing split and stats\n",
    "    split_stats_file = config.get('split_stats_file', 'split_and_stats.json')\n",
    "    if config.get('use_existing_split', False) and os.path.exists(split_stats_file):\n",
    "        print(f\"Loading existing split and stats from {split_stats_file}\")\n",
    "        train_indices, val_indices, train_mean, train_std = load_split_and_stats(split_stats_file)\n",
    "    else:\n",
    "        # Create a base dataset without augmentation for splitting and stats calculation\n",
    "        base_dataset = StabilityDataset(csv_file=config['train_csv'], \n",
    "                                        img_dir=config['train_img_dir'], \n",
    "                                        transform=transforms.ToTensor(),\n",
    "                                        augment=False,\n",
    "                                        use_quantized=config['use_quantized'],\n",
    "                                        additional_columns=config['additional_columns'])\n",
    "\n",
    "        # Split dataset into train and validation\n",
    "        dataset_size = len(base_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(config['val_ratio'] * dataset_size))\n",
    "        train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Calculate statistics for training set only\n",
    "        train_subset = Subset(base_dataset, train_indices)\n",
    "\n",
    "        print(\"Calculating training dataset statistics...\")\n",
    "        train_mean, train_std = calculate_stats(train_subset)\n",
    "        print(f\"Training dataset mean: {train_mean}\")\n",
    "        print(f\"Training dataset std: {train_std}\")\n",
    "\n",
    "        # Save split and stats\n",
    "        save_split_and_stats(train_indices, val_indices, train_mean, train_std, split_stats_file)\n",
    "        print(f\"Split and stats saved to {split_stats_file}\")\n",
    "\n",
    "    # Create transforms\n",
    "    normalize_transform = transforms.Normalize(mean=train_mean, std=train_std)\n",
    "    \n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform,\n",
    "    ])\n",
    "\n",
    "    # Create datasets with appropriate transforms\n",
    "    full_dataset = StabilityDataset(csv_file=config['train_csv'], \n",
    "                                    img_dir=config['train_img_dir'], \n",
    "                                    transform=base_transform,\n",
    "                                    augment=config['use_augmentation'],\n",
    "                                    use_quantized=config['use_quantized'],\n",
    "                                    additional_columns=config['additional_columns'],\n",
    "                                    target_column=config['target_column'])\n",
    "\n",
    "    # Get the number of categories for each additional feature\n",
    "    additional_features = full_dataset.get_feature_dimensions()\n",
    "    num_classes = full_dataset.get_target_dimension()\n",
    "\n",
    "\n",
    "    # Apply the split, ensuring augmented images stay with their original counterparts\n",
    "    if config['use_augmentation']:\n",
    "        train_indices = [i for idx in train_indices for i in range(idx * 4, (idx + 1) * 4)]\n",
    "        val_indices = [i for idx in val_indices for i in range(idx * 4, (idx + 1) * 4)]\n",
    "    \n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices[:len(val_indices)//4])  # Only use original images for validation\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=get_optimal_num_workers())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=get_optimal_num_workers())\n",
    "\n",
    "    # Initialize model, criterion, optimizer, and scheduler\n",
    "    if config['model'] == 'StabilityPredictor':\n",
    "        model = StabilityPredictor(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    elif config['model'] == 'EfficientAttentionNet':\n",
    "        model = EfficientAttentionNet(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    elif config['model'] == 'EfficientChannelAttentionNet':\n",
    "        model = EfficientChannelAttentionNet(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    elif config['model'] == 'ConvnextPredictor':\n",
    "        model = ConvnextPredictor(num_classes=num_classes, freeze_layers=config['freeze_layers'], additional_features=additional_features)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)\n",
    "    else:\n",
    "        print('Unrecognised model in config. Defaulting to StabilityPredictor (EfficientNet)')\n",
    "        model = StabilityPredictor(num_classes=num_classes, dropout_rate=config['dropout_rate'], additional_features=additional_features)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    print('Model: ' + config['model'])\n",
    "    print('Using quantized images: ' + str(config['use_quantized']))\n",
    "    print('Additional features:', ', '.join(f\"{k}: {v} categories\" for k, v in additional_features.items()))\n",
    "    print(f'Target feature: {config[\"target_column\"]} ({num_classes} categories)')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=config['lr_factor'], patience=config['lr_patience'], verbose=True)\n",
    "\n",
    "    # Train model\n",
    "    print('Training...')\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                        num_epochs=config['num_epochs'], patience=config['early_stopping_patience'], device=device)\n",
    "    \n",
    "\n",
    "    # Prediction on test set\n",
    "    test_dataset = StabilityDataset(csv_file=config['test_csv'],\n",
    "                                    img_dir=config['test_img_dir'],\n",
    "                                    transform=base_transform,\n",
    "                                    use_quantized=config['use_quantized'],\n",
    "                                    additional_columns=config['additional_columns'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=get_optimal_num_workers())\n",
    "\n",
    "    predictions, image_ids = predict(model, test_loader, device)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    predictions_save_path = f\"{config['model']}_predictions.csv\",\n",
    "    with open(predictions_save_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['id', config['target_column']])\n",
    "        for img_id, pred in zip(image_ids, predictions):\n",
    "            writer.writerow([int(img_id), int(pred)])\n",
    "    print(f\"Predictions saved to {predictions_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing split and stats from split_and_stats.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2473150/182210000.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_name = str(row[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientAttentionNet\n",
      "Using quantized images: False\n",
      "Additional features: \n",
      "Target feature: instability_type (3 categories)\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c0038c0e2846f7b238ac9cca377a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b06a67a20e4c38a091fb6b8dbb3c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 10.19 MiB is free. Process 2221263 has 5.25 GiB memory in use. Process 1244587 has 24.26 GiB memory in use. Process 1822121 has 14.80 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 377.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEfficientAttentionNet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_csv\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./COMP90086_2024_Project_train/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_stats_file\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_and_stats.json\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# File to save/load split and stats\u001b[39;00m\n\u001b[1;32m     23\u001b[0m }\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 649\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 649\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearly_stopping_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Prediction on test set\u001b[39;00m\n\u001b[1;32m    654\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m StabilityDataset(csv_file\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_csv\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    655\u001b[0m                                 img_dir\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_img_dir\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    656\u001b[0m                                 transform\u001b[38;5;241m=\u001b[39mbase_transform,\n\u001b[1;32m    657\u001b[0m                                 use_quantized\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_quantized\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    658\u001b[0m                                 additional_columns\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madditional_columns\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 418\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[1;32m    417\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 418\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Learning rate scheduler step\u001b[39;00m\n\u001b[1;32m    421\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
      "Cell \u001b[0;32mIn[1], line 459\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, data_loader, criterion, optimizer, device, is_training)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    457\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 459\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 190\u001b[0m, in \u001b[0;36mEfficientAttentionNet.forward\u001b[0;34m(self, x, *additional_inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39madditional_inputs):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Pass through the feature extractor (EfficientNet backbone) until the last feature map\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mefficientnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Extract convolutional features\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Apply spatial attention module to the feature maps\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_attention(features)\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torchvision/models/efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:405\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2104\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 2104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 10.19 MiB is free. Process 2221263 has 5.25 GiB memory in use. Process 1244587 has 24.26 GiB memory in use. Process 1822121 has 14.80 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 377.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'model': 'EfficientAttentionNet',\n",
    "    'train_csv': './COMP90086_2024_Project_train/train.csv',\n",
    "    'train_img_dir': './preprocessed_images/train',\n",
    "    'test_csv': './COMP90086_2024_Project_test/test.csv',\n",
    "    'test_img_dir': './preprocessed_images/test',\n",
    "    'additional_columns': [],\n",
    "    'target_column': 'instability_type',\n",
    "    'val_ratio': 0.1,\n",
    "    'use_augmentation': True,\n",
    "    'use_quantized': False,\n",
    "    'batch_size': 48,\n",
    "    'num_classes': 6,\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'lr_factor': 0.1,\n",
    "    'lr_patience': 3,\n",
    "    'freeze_layers': False,\n",
    "    'num_epochs': 30,\n",
    "    'early_stopping_patience': 6,\n",
    "    'use_existing_split': True,  # Set to True to use existing split and stats\n",
    "    'split_stats_file': 'split_and_stats.json'  # File to save/load split and stats\n",
    "}\n",
    "\n",
    "train_and_save(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
