{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fbff7-a2a6-4688-acee-bb8eed803bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7b42351feb30>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/COMP90086_Project/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import EfficientNet_V2_S_Weights\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import platform\n",
    "import multiprocessing\n",
    "\n",
    "class StabilityDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, target_feature, transform=None, augment=False, image_size=224, zoom_proportion=0.15):\n",
    "        self.stability_data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.target_feature = target_feature\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.image_size = image_size\n",
    "        self.zoom_proportion = zoom_proportion\n",
    "        self.augmented_indices = self._create_augmented_indices() if augment else None\n",
    "        \n",
    "        # Automatically determine class mapping\n",
    "        self.class_mapping = self._create_class_mapping()\n",
    "    \n",
    "    def _create_class_mapping(self):\n",
    "        unique_values = sorted(self.stability_data[self.target_feature].unique())\n",
    "        return {value: index for index, value in enumerate(unique_values)}\n",
    "\n",
    "\n",
    "    def _create_augmented_indices(self):\n",
    "        base_indices = list(range(len(self.stability_data)))\n",
    "        flipped_indices = [idx + len(self.stability_data) for idx in base_indices]\n",
    "        zoomed_indices = [idx + 2 * len(self.stability_data) for idx in base_indices]\n",
    "        zoomed_flipped_indices = [idx + 3 * len(self.stability_data) for idx in base_indices]\n",
    "        return base_indices + flipped_indices + zoomed_indices + zoomed_flipped_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stability_data) * 4 if self.augment else len(self.stability_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.augment:\n",
    "            original_idx = idx % len(self.stability_data)\n",
    "            augmentation = idx // len(self.stability_data)\n",
    "        else:\n",
    "            original_idx = idx\n",
    "            augmentation = 0\n",
    "\n",
    "        img_name = str(self.stability_data.iloc[original_idx, 0])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        target_value = self.stability_data.iloc[original_idx][self.target_feature]\n",
    "        target_class = self.class_mapping[target_value]\n",
    "\n",
    "        if self.augment:\n",
    "            if augmentation in [1, 3]:\n",
    "                image = image.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "            if augmentation in [2, 3]:\n",
    "                width, height = image.size\n",
    "                crop_size = int(min(width, height) * (1 - self.zoom_proportion))\n",
    "                left = (width - crop_size) // 2\n",
    "                top = (height - crop_size) // 2\n",
    "                right = left + crop_size\n",
    "                bottom = top + crop_size\n",
    "                image = image.crop((left, top, right, bottom))\n",
    "\n",
    "        image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(target_class, dtype=torch.long)\n",
    "\n",
    "class StabilityPredictor(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3):\n",
    "        super(StabilityPredictor, self).__init__()\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=weights)\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "        self.efficientnet.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "    \n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device, is_training=True)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer, device, is_training=False)\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience or val_acc > 99.99:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to run a single epoch (training or validation)\n",
    "def run_epoch(model, data_loader, criterion, optimizer, device, is_training=True):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\" if is_training else \"Validating\")\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100. * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_acc = 100. * correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Function to calculate dataset statistics\n",
    "def calculate_stats(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=100, num_workers=0, shuffle=False)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "    \n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "    return mean, std\n",
    "\n",
    "# Function to get optimal number of workers for data loading\n",
    "def get_optimal_num_workers():\n",
    "    # Windows can't do multiprocessing\n",
    "    if platform.system() == 'Windows':\n",
    "        return 0\n",
    "    else:\n",
    "        return multiprocessing.cpu_count()\n",
    "    \n",
    "def main(config):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Read the CSV file once\n",
    "    train_data = pd.read_csv(config['train_csv'])\n",
    "\n",
    "    # Automatically determine features and their number of classes\n",
    "    features = {}\n",
    "    for column in train_data.columns:\n",
    "        if column == 'cam_angle': #not in ['id', 'stable_height', 'shapeset']:  # Exclude non-feature columns\n",
    "            unique_values = train_data[column].nunique()\n",
    "            features[column] = unique_values\n",
    "\n",
    "    for feature, num_classes in features.items():\n",
    "        print(f\"\\nTraining model for feature: {feature}\")\n",
    "        \n",
    "        # Update config for the current feature\n",
    "        config['target_feature'] = feature\n",
    "        config['num_classes'] = num_classes\n",
    "        config['model_save_path'] = f'stability_predictor_{feature}.pth'\n",
    "        config['predictions_save_path'] = f'predictions_{feature}.csv'\n",
    "\n",
    "        # Create datasets\n",
    "        full_dataset = StabilityDataset(csv_file=config['train_csv'], \n",
    "                                        img_dir=config['train_img_dir'], \n",
    "                                        target_feature=config['target_feature'],\n",
    "                                        transform=transforms.ToTensor(),\n",
    "                                        augment=False,\n",
    "                                        image_size=config['image_size'])\n",
    "        \n",
    "        # Split dataset\n",
    "        dataset_size = len(full_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(config['val_ratio'] * dataset_size))\n",
    "        train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        train_subset = Subset(full_dataset, train_indices)\n",
    "\n",
    "        print(\"Calculating training dataset statistics...\")\n",
    "        train_mean, train_std = calculate_stats(train_subset)\n",
    "\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=train_mean, std=train_std),\n",
    "        ])\n",
    "\n",
    "        train_dataset = StabilityDataset(csv_file=config['train_csv'], \n",
    "                                         img_dir=config['train_img_dir'], \n",
    "                                         target_feature=config['target_feature'],\n",
    "                                         transform=data_transform,\n",
    "                                         augment=config['use_augmentation'],\n",
    "                                         image_size=config['image_size'],\n",
    "                                         zoom_proportion=config['zoom_proportion'])\n",
    "        train_dataset = Subset(train_dataset, [i for i in range(len(train_dataset)) if i % len(full_dataset) in train_indices])\n",
    "\n",
    "        val_dataset = StabilityDataset(csv_file=config['train_csv'], \n",
    "                                       img_dir=config['train_img_dir'], \n",
    "                                       target_feature=config['target_feature'],\n",
    "                                       transform=data_transform,\n",
    "                                       augment=False,\n",
    "                                       image_size=config['image_size'])\n",
    "        val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n",
    "\n",
    "        # Initialize model, criterion, optimizer, and scheduler\n",
    "        model = StabilityPredictor(num_classes=config['num_classes'], dropout_rate=config['dropout_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=config['lr_factor'], patience=config['lr_patience'])\n",
    "\n",
    "        # Train model\n",
    "        print('Training...')\n",
    "        model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                            num_epochs=config['num_epochs'], patience=config['early_stopping_patience'], device=device)\n",
    "\n",
    "        torch.save(model.state_dict(), config['model_save_path'])\n",
    "        print(f\"Model for {feature} saved to {config['model_save_path']}\")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'train_csv': './COMP90086_2024_Project_train/train.csv',\n",
    "    'train_img_dir': './COMP90086_2024_Project_train/train',\n",
    "    'test_csv': './COMP90086_2024_Project_test/test.csv',\n",
    "    'test_img_dir': './COMP90086_2024_Project_test/test',\n",
    "    'image_size': 224,\n",
    "    'val_ratio': 0.1,\n",
    "    'use_augmentation': False,\n",
    "    'zoom_proportion': 0.15,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': get_optimal_num_workers(),\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'lr_factor': 0.1,\n",
    "    'lr_patience': 2,\n",
    "    'num_epochs': 30,\n",
    "    'early_stopping_patience': 3,\n",
    "}\n",
    "\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1aa73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cuda:0\n",
      "INFO:root:Loaded 1920 test samples and 7680 train samples\n",
      "INFO:root:Features to predict: ['shapeset', 'type', 'total_height', 'instability_type', 'cam_angle']\n",
      "INFO:root:TestStabilityDataset initialized with 1920 samples\n",
      "INFO:root:Predicting shapeset...\n",
      "INFO:root:Number of classes for shapeset: 2\n",
      "INFO:root:Starting predictions for shapeset\n",
      "Predicting: 100%|██████████| 60/60 [00:07<00:00,  7.87it/s]\n",
      "INFO:root:Completed predictions for shapeset. Got 1920 predictions.\n",
      "INFO:root:Stored predictions for shapeset\n",
      "INFO:root:Predicting type...\n",
      "INFO:root:Number of classes for type: 2\n",
      "INFO:root:Starting predictions for type\n",
      "Predicting: 100%|██████████| 60/60 [00:07<00:00,  8.03it/s]\n",
      "INFO:root:Completed predictions for type. Got 1920 predictions.\n",
      "INFO:root:Stored predictions for type\n",
      "INFO:root:Predicting total_height...\n",
      "INFO:root:Number of classes for total_height: 5\n",
      "INFO:root:Starting predictions for total_height\n",
      "Predicting: 100%|██████████| 60/60 [00:08<00:00,  7.03it/s]\n",
      "INFO:root:Completed predictions for total_height. Got 1920 predictions.\n",
      "INFO:root:Stored predictions for total_height\n",
      "INFO:root:Predicting instability_type...\n",
      "INFO:root:Number of classes for instability_type: 3\n",
      "INFO:root:Starting predictions for instability_type\n",
      "Predicting: 100%|██████████| 60/60 [00:08<00:00,  6.82it/s]\n",
      "INFO:root:Completed predictions for instability_type. Got 1920 predictions.\n",
      "INFO:root:Stored predictions for instability_type\n",
      "INFO:root:Predicting cam_angle...\n",
      "INFO:root:Number of classes for cam_angle: 2\n",
      "INFO:root:Starting predictions for cam_angle\n",
      "Predicting: 100%|██████████| 60/60 [00:09<00:00,  6.14it/s]\n",
      "INFO:root:Completed predictions for cam_angle. Got 1920 predictions.\n",
      "INFO:root:Stored predictions for cam_angle\n",
      "INFO:root:Added predictions for shapeset to result DataFrame\n",
      "INFO:root:Added predictions for type to result DataFrame\n",
      "INFO:root:Added predictions for total_height to result DataFrame\n",
      "INFO:root:Added predictions for instability_type to result DataFrame\n",
      "INFO:root:Added predictions for cam_angle to result DataFrame\n",
      "INFO:root:Predictions saved to test_imputed.csv\n",
      "INFO:root:Sample of predictions:\n",
      "     id  shapeset  type  total_height  instability_type  cam_angle\n",
      "0    95         1     1             5                 0          1\n",
      "1   706         2     1             6                 2          1\n",
      "2  2854         2     1             3                 2          1\n",
      "3  3093         1     2             6                 1          2\n",
      "4  4283         1     2             6                 1          1\n",
      "INFO:root:Value counts for shapeset:\n",
      "shapeset\n",
      "2    1277\n",
      "1     643\n",
      "Name: count, dtype: int64\n",
      "INFO:root:Value counts for type:\n",
      "type\n",
      "1    968\n",
      "2    952\n",
      "Name: count, dtype: int64\n",
      "INFO:root:Value counts for total_height:\n",
      "total_height\n",
      "6    596\n",
      "5    462\n",
      "4    378\n",
      "3    320\n",
      "2    164\n",
      "Name: count, dtype: int64\n",
      "INFO:root:Value counts for instability_type:\n",
      "instability_type\n",
      "1    937\n",
      "0    507\n",
      "2    476\n",
      "Name: count, dtype: int64\n",
      "INFO:root:Value counts for cam_angle:\n",
      "cam_angle\n",
      "1    1440\n",
      "2     480\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class TestStabilityDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, image_size=224):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        logging.info(f\"TestStabilityDataset initialized with {len(self.data)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = str(self.data.iloc[idx]['id'])\n",
    "            img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                logging.error(f\"Image file not found: {img_path}\")\n",
    "                return None, img_name\n",
    "\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, img_name\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing image at index {idx}: {e}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return None, str(self.data.iloc[idx]['id'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    if len(batch) == 0:\n",
    "        return torch.Tensor(), []\n",
    "    return torch.stack([item[0] for item in batch]), [item[1] for item in batch]\n",
    "\n",
    "def load_model(model_path, num_classes):\n",
    "    try:\n",
    "        model = StabilityPredictor(num_classes)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'), weights_only=True))\n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model from {model_path}: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    predictions = []\n",
    "    img_names = []\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for inputs, names in tqdm(data_loader, desc=\"Predicting\"):\n",
    "                if inputs.numel() == 0:\n",
    "                    continue\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                img_names.extend(names)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during prediction: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "    return predictions, img_names\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Configuration\n",
    "    test_csv = './COMP90086_2024_Project_test/test.csv'\n",
    "    test_img_dir = './COMP90086_2024_Project_test/test'\n",
    "    train_csv = './COMP90086_2024_Project_train/train.csv'\n",
    "    batch_size = 32\n",
    "    num_workers = 0  # Set to 0 to debug DataLoader issues\n",
    "    image_size = 224\n",
    "\n",
    "    # Load test and train data\n",
    "    test_data = pd.read_csv(test_csv)\n",
    "    train_data = pd.read_csv(train_csv)\n",
    "    logging.info(f\"Loaded {len(test_data)} test samples and {len(train_data)} train samples\")\n",
    "\n",
    "    # Prepare data transform\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Dictionary to store predictions\n",
    "    all_predictions = {}\n",
    "\n",
    "    # Get all features except 'id' and 'stable_height' from train_data\n",
    "    features = [col for col in train_data.columns if col not in ['id', 'stable_height']]\n",
    "    logging.info(f\"Features to predict: {features}\")\n",
    "\n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = TestStabilityDataset(csv_file=test_csv, \n",
    "                                        img_dir=test_img_dir, \n",
    "                                        transform=data_transform,\n",
    "                                        image_size=image_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                             num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    # Iterate through each feature\n",
    "    for feature in features:\n",
    "        logging.info(f\"Predicting {feature}...\")\n",
    "        \n",
    "        # Load the corresponding model\n",
    "        model_path = f'stability_predictor_{feature}.pth'\n",
    "        if not os.path.exists(model_path):\n",
    "            logging.warning(f\"Model for {feature} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load model\n",
    "        num_classes = len(train_data[feature].unique())\n",
    "        logging.info(f\"Number of classes for {feature}: {num_classes}\")\n",
    "        model = load_model(model_path, num_classes)\n",
    "        if model is None:\n",
    "            logging.warning(f\"Failed to load model for {feature}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Make predictions\n",
    "        logging.info(f\"Starting predictions for {feature}\")\n",
    "        predictions, img_names = predict(model, test_loader, device)\n",
    "        logging.info(f\"Completed predictions for {feature}. Got {len(predictions)} predictions.\")\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            logging.warning(f\"No predictions were made for {feature}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Convert numeric predictions back to original classes\n",
    "        class_mapping = {i: class_name for i, class_name in enumerate(sorted(train_data[feature].unique()))}\n",
    "        predictions = [class_mapping[pred] for pred in predictions]\n",
    "\n",
    "        # Store predictions\n",
    "        all_predictions[feature] = dict(zip(img_names, predictions))\n",
    "        logging.info(f\"Stored predictions for {feature}\")\n",
    "\n",
    "    # Create a new dataframe with 'id' from the original test data\n",
    "    result_df = pd.DataFrame({'id': test_data['id']})\n",
    "\n",
    "    # Add predictions to the result dataframe\n",
    "    for feature in features:\n",
    "        if feature in all_predictions:\n",
    "            result_df[feature] = result_df['id'].astype(str).map(all_predictions[feature])\n",
    "            logging.info(f\"Added predictions for {feature} to result DataFrame\")\n",
    "        else:\n",
    "            logging.warning(f\"No predictions found for {feature}\")\n",
    "\n",
    "    # Save the imputed test data\n",
    "    output_path = 'test_imputed.csv'\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "    # Log a sample of the predictions\n",
    "    logging.info(f\"Sample of predictions:\\n{result_df.head()}\")\n",
    "\n",
    "    # Log value counts for each feature\n",
    "    for feature in features:\n",
    "        logging.info(f\"Value counts for {feature}:\\n{result_df[feature].value_counts()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
